{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pandas as pd\n",
    "from lexnlp.extract.en.entities import nltk_re\n",
    "from lexnlp.nlp.en.segments import sentences\n",
    "from lexnlp.utils import parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"..\\data\\interim\\sfcr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f)) and f[-3:]=='txt']\n",
    "documents = []\n",
    "for file_name in txt_files:\n",
    "    file = open(join(DATA_PATH, file_name), \"rb\")\n",
    "    text = file.read().decode('utf-8')\n",
    "    file.close()\n",
    "    #text = text.replace(\"\\n\", \" \")\n",
    "    documents.append(text)\n",
    "    \n",
    "pickle_files = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f)) and f[-6:]=='pickle']\n",
    "df = pd.DataFrame()\n",
    "for file_name in pickle_files:\n",
    "    df = df.append(pd.read_pickle(join(DATA_PATH, file_name)), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of documents: \" + str(len(documents)))\n",
    "print(\"Number of sentences: \" + str(len(df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexnlp.nlp.en.segments import titles, sections, paragraphs, pages\n",
    "from dnbnlp.nlp.en.segments import solvency2_titles\n",
    "from dnbnlp.nlp.en.segments import solvency2_document_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(documents)):\n",
    "    documents[idx] = documents[idx].replace('\\uf0b7', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf0a7', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf0fc', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf00c', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf00d', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf020', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in range(len(documents[2:])):\n",
    "#     print(\"Document: \" + str(txt_files[item]) + \" (\" + str(item) + \")\")\n",
    "#     print(\"Sections: \" + str(list(sections.get_sections(documents[item], score_threshold=0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexnlp.extract.common import fact_extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = list(sections.get_sections(documents[14], score_threshold = 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#a[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dnbnlp\n",
    "f = \"..//dictionary//en//terminology.csv\"\n",
    "config_list = list(src.extract.en.solvency2_terms.load_entities_dict_by_path(f))\n",
    "\n",
    "b = fact_extracting.FactExtractor\n",
    "b.ensure_parser_arguments_en(config_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = \"There was fifty one of them. On May 5 the Solvency Capital Requirement. is 5 million.\"\n",
    "# sent = sent.replace(\".\", \" .\")\n",
    "sent = \"Totals 6,576,210.93 Loan 12/11/2007          10,000,000.00\"\n",
    "c = b.parse_text(sent, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent[8:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"May 16, 2015   100.0\"\n",
    "#list(lexnlp.extract.common.date_parsing.datefinder.extract_date_strings(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "base_date = datetime.now().replace(\n",
    "        day=1, month=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    # Find potential dates\n",
    "date_finder = lexnlp.extract.en.dates.DateFinder(base_date=base_date)\n",
    "#list(date_finder.find_dates(sent))\n",
    "list(lexnlp.extract.en.dates.get_raw_dates(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_dates = [(date_string, index, date_props) for date_string, index, date_props in\n",
    "                      date_finder.extract_date_strings(sent, strict=False)]\n",
    "possible_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import *\n",
    "from datetime import *\n",
    "now = parse(\"May 16, 2016 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexnlp.utils.lines_processing import parsed_text_corrector\n",
    "from lexnlp.nlp.en.segments import sentences, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = parsed_text_corrector.ParsedTextCorrector()\n",
    "text = s.correct_if_corrupted(documents[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
