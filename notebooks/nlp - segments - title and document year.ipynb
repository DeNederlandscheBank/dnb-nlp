{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pandas as pd\n",
    "from lexnlp.extract.en.entities import nltk_re\n",
    "from lexnlp.nlp.en.segments import sentences\n",
    "from lexnlp.utils import parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"..\\data\\interim\\sfcr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f)) and f[-3:]=='txt']\n",
    "documents = []\n",
    "for file_name in txt_files:\n",
    "    file = open(join(DATA_PATH, file_name), \"rb\")\n",
    "    text = file.read().decode('utf-8')\n",
    "    file.close()\n",
    "    #text = text.replace(\"\\n\", \" \")\n",
    "    documents.append(text)\n",
    "    \n",
    "pickle_files = [f for f in listdir(DATA_PATH) if isfile(join(DATA_PATH, f)) and f[-6:]=='pickle']\n",
    "df = pd.DataFrame()\n",
    "for file_name in pickle_files:\n",
    "    df = df.append(pd.read_pickle(join(DATA_PATH, file_name)), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents: \" + str(len(documents)))\n",
    "print(\"Number of sentences: \" + str(len(df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexnlp.nlp.en.segments import titles\n",
    "from dnbnlp.nlp.en.segments import solvency2_titles\n",
    "from dnbnlp.nlp.en.segments import solvency2_document_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(documents)):\n",
    "    documents[idx] = documents[idx].replace('\\uf0b7', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf0a7', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf0fc', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf00c', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf00d', \"\")\n",
    "    documents[idx] = documents[idx].replace('\\uf020', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models voor title and document year locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvency2_titles.build_model(\"..\\\\dnbnlp\\\\nlp\\\\en\\\\segments\\\\solvency2_titles_model.csv\", DATA_PATH)\n",
    "solvency2_document_year.build_model(\"..\\\\dnbnlp\\\\nlp\\\\en\\\\segments\\\\solvency2_document_year_model.csv\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload title and document year locators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvency2_document_year.SECTION_SEGMENTER_MODEL = joblib.load(os.path.join(solvency2_document_year.MODULE_PATH, \"./solvency2_document_year_locator.pickle\"))\n",
    "solvency2_titles.SECTION_SEGMENTER_MODEL = joblib.load(os.path.join(solvency2_titles.MODULE_PATH, \"./solvency2_title_locator.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(len(documents)):\n",
    "    print(\"Document           : \" + str(txt_files[item]) + \" : \" + str(item))\n",
    "    print(\"Predicted title    : \" + str(list(solvency2_titles.get_titles(documents[item], score_threshold=0.5))))\n",
    "    print(\"Predicted year     : \" + str(list(solvency2_document_year.get_document_years(documents[item], score_threshold = 0.1))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_data = solvency2_document_year.build_document_document_year_features(documents[0])\n",
    "# # Predict title lines\n",
    "# predicted_lines = solvency2_document_year.SECTION_SEGMENTER_MODEL.predict_proba(feature_data)\n",
    "# predicted_df = pd.DataFrame(predicted_lines, columns=[\"prob_false\", \"prob_true\"])\n",
    "# title_lines = predicted_df.loc[predicted_df[\"prob_true\"] > 0.1, :].index.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
